\documentclass[12pt, 
	a4paper, 
	%landscape, 
	oneside, 
	leqno]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[left=2cm, right=2cm, top=2.5cm, bottom=3.5cm]{geometry}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath, cancel}
\usepackage{thmtools}
\usepackage{url}
\usepackage{color}
\usepackage{listings}

\definecolor{lightgray}{rgb}{.9,.9,.9}
\definecolor{darkgray}{rgb}{.4,.4,.4}
\definecolor{purple}{rgb}{0.65, 0.12, 0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}

\lstset{
   language=JavaScript,
   backgroundcolor=\color{lightgray},
   extendedchars=true,
   basicstyle=\footnotesize\ttfamily,
   showstringspaces=false,
   showspaces=false,
   numbers=left,
   numberstyle=\footnotesize,
   numbersep=9pt,
   tabsize=2,
   breaklines=true,
   showtabs=false,
   captionpos=b
}

\begin{document}

\begin{titlepage}
\begin{center}

\begin{Huge}

\textbf{Grundlagenkurs Data Science}
\newline\newline
\end{Huge}

\begin{LARGE}
Forschungstagebuch \\
<<Data Science Project>> \\
\end{LARGE}
\vfill
\textbf{Working Title \\ Explorative Studie über Twitter in Berlin im Zeitraum 25.05.2017 - 08.06.2017}

\end{center}
\vfill
Laura Hartgers s0556238 \\
Adrian Saiz s0554249 \\
Christoph Stach s0555912 \\
\\
betreut durch Prof. Dr. Christin Schmidt \\
Hochschule für Technik und Wirtschaft Berlin
\end{titlepage}

\setcounter{tocdepth}{2} %Tiefe des Inhaltsverzeichnis
\tableofcontents
\pagebreak

\chapter{Problemformulierung}

Dieses Projekt umfasst eine explorative / hypothesensuchende Studie nach dem Inhalt von Twitter.
\\
Die von Twitter bereitgestellte API bietet die Mögichkeit zu suchen auf followings, locations und trackTerms.
\\
Das Team interessiert sich momentan für politische Themen, sowie die AfD, NPD, Trump und radikalisierung. 
\\
Das Team hat sich entschieden auf eine noch festzulegende Schlagwörterliste zu filtern, und dabei sowohl auf followings als die trackTerms zu filtern. Erzielt wird das eheben der Tweets aller User die im Zeitbereich der Datenerhebung über die ausgewählte Themen twittern als die Tweets der User die bestimmte Twitterprofile folgen.

%Darstellung der empirischen Untersuchbarkeit / Angemessenheit 
%des Arbeitsaufwandes 
%Hinweis:  
%Vorläufige Untersuchungsideen sind unbrauchbar, ... 
%... wenn unklar bleibt, was der eigentliche Gegenstand der 
%Untersuchung sein soll, oder 
%...der Gegenstand, auf den sich die Untersuchung bezieht, so 
%vielschichtig ist, dass sich aus ihm viele unterschiedliche 
%Fragestellungen ableiten lassen. 


\subsection*{Teilfragen}


Socialgraph(Laura) -> ER-Diag. -> Tabellen erstellen -> JSON Tweets parsen und nach SQL überführen
\\ \\
Hashtag-Wordcloud(1. Christoph): Filterung der Tweets nach Hashtags -> Zählen der Vorkommnisse der Hashtags
\\ \\
Timeline-Exploration (2. Christoph): Darstellung der Tweets pro Tag pro (min/sec/stunde) 
\\ \\ 
Welche Sprachen werden auf Twitter gesprochen (Irgendjemand)

\chapter{Theorie}

Literaturstudium (Stand der Wissenschaft) \\

Explorativ/ Hypothesensuchend: Was gibt es schon? \\

Erkundung (Warum dieses Thema? Lücke in Wissenschaft?, 
eigenes Interesse?, Zufall?, .... ) \\
- Eigenes Interesse in ausprobieren von Methoden die im Data Science Bereich benutzt werden. \\
-Interessant weil kombinierbar mit aktuelle Programmierkenntnisse.

\chapter{Methodologie / Untersuchungsplanung}

%Wie gehen sie vor und warum? Welche Werkzeuge nutzen Sie? 

%Datenbeschaffung (Qualität und Quantität der Daten mit 
%Darstellung weiterer Quellen / Erhebungsmethoden?) 
%Erhebungsart  
%Stichprobe 
%Auswertungsmethoden 

%Ethische Kriterien berücksichtigen (vgl. Data Privacy) 
%Missbrauch personenbezogener Daten (Anonymisierung, 
%Pseudonymisierung)  

%Beeinträchtigung der Untersuchungsteilnehmer

\chapter{Herausforderungen}

\chapter{Untersuchungsdurchführung}

\chapter{Ergebnisse}

\chapter{Quellen}

\chapter{Team Working Agreements}

1. Jede Woche Montag 15.30 ist Teammeeting.\\
\\
2. Verantwortungsvoll mit dem Server umgehen und jeder dokumentiert selbst was er darauf macht. \\
\\
3. Aufgaben werden etwa gleichmäßig über das Team verteilt. Stärken und Schwächen werden dabei berücksichtigt. Jeder hat Vertantwortung für seine eigene Aufgaben. Teammitglieder sprechen ein Teammitglied, das seine Aufgabe nicht in der abgesprochene Zeit erledigt hat, spätenstens einen Tag später darauf an.\\
\\
4. Projektstrukturierung / Aufgabenverteilung mit Trello. 

\chapter{Diskussion}

\chapter{Code}

\section{MongoDB - Praktische Queries}

\subsection{Vorkommen aller Sprachen}
\begin{lstlisting}[language=JavaScript]
db.getCollection('tweetsGermanStopWords').aggregate([
    { 
        $group: { 
            _id: '$lang',
            occurrences: {
                $sum: 1
            }
        }
    },
    {
        $sort: {
            count: -1
        }
    }
])
\end{lstlisting}

\subsection{Vorkommen aller Hashtags}
\begin{lstlisting}[language=JavaScript]
db.getCollection('tweetsBerlinGeo').aggregate([  
    { 
        $unwind: {
            path: "$entities.hashtags"
        }
    },
    { 
        $group: { 
            _id : "$entities.hashtags.text",
            occurrences: {
                $sum: 1
            }
        }
    },
    {
        $sort: {
            occurrences: -1 
        }
    },
    {
        $limit: 10
    }
])
\end{lstlisting}

\chapter{Tagebücher}

\section{Christoph Stach}

Ich fasse hier mal zusammen was ich bisher so gemacht habe.
Damit können wir uns später besser einen Überblick verschaffen und gegebenfalls eine Timeline erstellen.

\subsection*{Recherche}

\begin{itemize}
  \item \textbf{27.04.2017:} Wo bekommen wir die Daten her?
  \item \textbf{27.04.2017:} Twitter Streaming API streamt Tweets sobald Sie getweeted werden zu einem Client
  \item \textbf{27.04.2017:} Entscheidung: Hosebird Client (hbc), weil offiziell von Twitter entwickelt
  \item \textbf{08.04.2017:} Wie kann man Text analysieren? Ggf. NLP Techniken anwenden
  \item \textbf{11.04.2017:} Testen der NLP Library http://www.spacy.io/
  \item \textbf{11.04.2017:} Erfolreiche Extrahierung von \textit{Named Entities} mit SpaCy aus Tweets
  \item \textbf{14.05.2017:} Interessante Studie, die ähnlich unserer ist: http://www.ling.uni-potsdam.de/~scheffler/twitter/
  \item \textbf{24.05.2017:} Research über Darstellungsoptionen -> jQCloud, plotly.js
\end{itemize}

\subsection*{DataMiner}

\begin{itemize}
  \item \textbf{27.04.2017:} Erste Erstellung eines lauffähigen Prototypens welcher sich auf die Twitter Streaming API verbindet und Tweets empfängt
  \item \textbf{08.05.2017:} Consumer können sich jetzt auf mit dem Twitter Client verbinden und auf auftretende Tweets reagieren
  \item \textbf{08.05.2017:} Implementierung eines MongoDB Consumers, mit MongoDB lassen sich einfach JSON Objekte speichern und bei bedarf durchsuchen. Die Twitter API sendet Tweets im JSON Format, von daher erweist sich MongoDB als idealer initialer Datenspeicher.
  \item \textbf{14.05.2017:} Implementierung für den Locationfilter
  \item \textbf{14.05.2017:} Erstellung eines Shellscripts um den Twitter Client auf dem Server im Hintergrund laufen zu lassen
  \item \textbf{15.05.2017:} Consumer sind jetzt per config.yml aktivierbar
  \item \textbf{16.05.2017:} Verbesserung des Twitter Clients und Unterschützung für mehrfache Locationfilter
  \item \textbf{18.05.2017:} Ich habe DataMining für Tweets in der Georegion Berlin um 03:15 gestarted.
  \item \textbf{03.06.2017:} DataMiner mit SpringBoot weiter entwickelt. Linux service implementiert.
\end{itemize}

\subsection*{Berechnung der Daten}
  \item \textbf{24.05.2017:} Erstellung eine Spring Boot Web Service welche die Daten von der MongoDB berechnet und umformt. Erster algorythmus für most recent Tags (dauert bei großen Datenmengen 30 sec)
  \item \textbf{25.05.2017:} Optimierung des "Most recent tag" - Algorythmus, wird jetzt von MongoDB berechnet mit MongoDb-Aggrigation-Framework und dauert nur noch 1 sec.
  \item \textbf{29.05.2017:} Implementierung zwei neuer Endpunkt. Tweets pro Stunde (0-23) und Tweets pro Wochentag (Montag-Sonntag)

\subsection*{Visualisierung}
  \item \textbf{24.05.2017:} Einbinden der jQCloud auf meiner Internetseite und Upload von Testdaten
  \item \textbf{28.05.2017:} Erstellung eines Diagrams mit Chart.js für die Abbildung der Tweets pro Wochentag, vorläufig nur mit Testdaten

\subsection*{Server}

\begin{itemize}
  \item \textbf{14.05.2017:} Upload des Twitter Clients auf den Server um einen Testlauf zu starten (Tweets in Berlin)
  \item \textbf{25.05.2017:} Zwischen 21 und 23 Uhr kurzer Serverausfall
  \item \textbf{02.06.2017:} DataMiner gestoppt (ka warum) um 01:58
  \item \textbf{04.06.2017:} DataMiner gestarted und als Service installiert um 14:00 Uhr
\end{itemize}

\end{document}
